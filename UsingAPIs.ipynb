{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guide to Using APIs with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the required libraries\n",
    "\n",
    "**MUST DO THIS FIRST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint, requests, json\n",
    "from pandas.io.json import json_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RapidAPI\n",
    "The following APIs are from the RapidAPI website so they all have the same basic format which is outlined in the code block below. Some sources have multiple API options. For the API key, sign up for a free account with RapidAPI and the keys will be generated.\n",
    "\n",
    "### Instagram\n",
    "*Option 1*: url = \"https://instagramdimashirokovv1.p.rapidapi.com/user/{username}\"  \n",
    "*Option 2*: url = \"https://instagram29.p.rapidapi.com/user/{username}\"\n",
    "\n",
    "### Reddit\n",
    "In addition to the **headers** argument, reddit requires a **params** argument. \n",
    "\n",
    "*For posts*: url = \"https://socialgrep.p.rapidapi.com/search/posts\"  \n",
    "*For comments*: url = \"https://socialgrep.p.rapidapi.com/search/comments\"\n",
    "\n",
    "### Hoaxy\n",
    "\n",
    "\n",
    "### Google Trends\n",
    "\n",
    "## SerpApi\n",
    "These APIs are from SerpApi, which have a specific format that I've adapted.\n",
    "\n",
    "### Google Scholar Search\n",
    "\n",
    "### YouTube\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Turning JSON data into a dataframe\n",
    "\n",
    "Two easy ways to do this:  \n",
    "`pd.json_normalize` - this is good for nested dictionaries, like the Reddit one below.  \n",
    "`pd.read_json` - this is good for flat dictionaries, like the Instagram one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "url = \"https://instagramdimashirokovv1.p.rapidapi.com/user/{username}\"\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-key': \"api-key\",\n",
    "    'x-rapidapi-host': \"InstagramdimashirokovV1.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "print(response.text)\n",
    "\n",
    "# Option 2\n",
    "#url = \"https://instagram29.p.rapidapi.com/user/{username}\"\n",
    "\n",
    "#headers = {\n",
    "#    'x-rapidapi-key': \"api-key\",\n",
    "#    'x-rapidapi-host': \"instagram29.p.rapidapi.com\"\n",
    "#    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data and see what variables are there\n",
    "my_insta = json.loads(response.text)\n",
    "my_insta.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the data into a dataframe\n",
    "insta_df = pd.read_json(my_insta)\n",
    "insta_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "url = \"https://socialgrep.p.rapidapi.com/search/posts\"\n",
    "\n",
    "querystring = {\"query\":\"conspiracy\"}\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-key': \"api-key\",\n",
    "    'x-rapidapi-host': \"socialgrep.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "\n",
    "# Option 2\n",
    "#url = \"https://reddit3.p.rapidapi.com/subreddit\"\n",
    "\n",
    "#querystring = {\"url\":\"https://www.reddit.com/r/funny\"}\n",
    "\n",
    "#headers = {\n",
    "#    'x-rapidapi-key': \"api-key\",\n",
    "#    'x-rapidapi-host': \"reddit3.p.rapidapi.com\"\n",
    "#    }\n",
    "\n",
    "reddit = json.loads(response.text)\n",
    "reddit.keys()\n",
    "\n",
    "# Replace `record_path` with whatever variable you see after entering the .keys() command\n",
    "reddit_df = pd.json_normalize(reddit, record_path = ['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function actually works better thank the regular print() for printing out nested lists like this\n",
    "pprint.pprint(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes the text and puts it into one string for text analytic purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "for post in reddit_df['title']:\n",
    "    text.append(post)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the dataframe to a csv file\n",
    "reddit_df.to_csv('reddit_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Weather Map Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For current weather data\n",
    "url = \"https://community-open-weather-map.p.rapidapi.com/weather\"\n",
    "\n",
    "querystring = {\"q\":\"buffalo,us\", \"units\":\"imperial\"}\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-key': \"api-key\",\n",
    "    'x-rapidapi-host': \"community-open-weather-map.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "weather_dict = json.loads(response.text)\n",
    "\n",
    "weather_df = pd.json_normalize(weather_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a glimpse of the data with head() and the types of variables with dtypes()\n",
    "weather_df.head()\n",
    "weather_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing out what the current weather is like\n",
    "weather = weather_dict['weather']\n",
    "print('The current weather condition in %s is %s.' % (weather_dict['name'],weather[0]['description']))\n",
    "print('The current temperature is %s degrees Farenheit with %s percent humidity.' % (weather_dict['main']['temp'], weather_dict['main']['humidity']))\n",
    "print('But it feels like it is %s degrees....' % (weather_dict['main']['feels_like']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert the unix timestamp into human readable format\n",
    "import time, datetime\n",
    "print(datetime.datetime.fromtimestamp(weather_df['sys.sunrise']))\n",
    "print(datetime.datetime.fromtimestamp(weather_df['sys.sunset']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only columns of interest\n",
    "weather_df[['name','main.temp','main.temp_min','main.temp_max','main.feels_like','main.humidity']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hoaxy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://api-hoaxy.p.rapidapi.com/articles\"\n",
    "\n",
    "querystring = {\"query\":\"Trump OR Biden AND date_published:[2020-03-30 TO 2020-12-31]\",\"use_lucene_syntax\":\"true\",\"sort_by\":\"relevant\"}\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-key': \"api-key\",\n",
    "    'x-rapidapi-host': \"api-hoaxy.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "hoaxy = json.loads(response.text)\n",
    "hoax_df = pd.json_normalize(hoaxy, record_path = ['articles'])\n",
    "\n",
    "\n",
    "print(hoaxy.keys())\n",
    "print(hoax_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Trends Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://google-news.p.rapidapi.com/v1/search\"\n",
    "\n",
    "querystring = {\"q\":\"news\",\"country\":\"US\",\"lang\":\"en\"}\n",
    "\n",
    "headers = {\n",
    "    'x-rapidapi-key': \"api-key\",\n",
    "    'x-rapidapi-host': \"google-news.p.rapidapi.com\"\n",
    "    }\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers, params=querystring)\n",
    "\n",
    "searchResults = json.loads(response.text)\n",
    "df = pd.json_normalize(searchResults, record_path = ['articles'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(searchResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(searchResults, record_path = ['articles'])\n",
    "df['published'] = pd.to_datetime(df['published'])\n",
    "df = df.sort_values(by='published', ascending=False)\n",
    "df.head(10)\n",
    "df.to_csv('google_news_data.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YouTube Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://serpapi.com/search.json?engine=youtube'\n",
    "\n",
    "params = {\n",
    "  \"engine\": \"youtube\",\n",
    "  \"search_query\": \"Official FBI\",\n",
    "  \"api_key\": \"449638a07800c4d6845976740a7a1f772ffd3e560d201736f6f1fa9bf04f6fcf\"\n",
    "}\n",
    "\n",
    "results = requests.get(url, params)\n",
    "#video_results = results['video_results']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoresults = json.loads(results.text)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eed0ff3c3f7dbdf7703bfa80ad1ad3806aeb8f63baa04d3527d904229c654093"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
